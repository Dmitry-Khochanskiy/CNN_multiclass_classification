{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as tv\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylogs = logging.getLogger()\n",
    "mylogs.setLevel(logging.INFO)\n",
    "# Logging to file\n",
    "file = logging.FileHandler(\"training.log\", mode='w')\n",
    "file.setLevel(logging.INFO)\n",
    "fileformat = logging.Formatter(\"%(asctime)s:%(levelname)s:%(message)s\",datefmt=\"%H:%M:%S\")\n",
    "file.setFormatter(fileformat)\n",
    "mylogs.addHandler(file)\n",
    "# Logging to console\n",
    "stream = logging.StreamHandler()\n",
    "stream.setLevel(logging.INFO)\n",
    "mylogs.addHandler(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mri_classification_ResNet18.pt\"\n",
    "n_of_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    ''' Loads a model and its state dict. Accepts path to the model, returns the model and all associated data.\n",
    "    Default CNN type is resnet18 '''\n",
    "    loaded_model = {}\n",
    "    saved_model = torch.load(f'{model_path}')\n",
    "    loaded_model[\"labeling mode\"] = saved_model['labeling_mode']\n",
    "    loaded_model['img paths'] =  saved_model['img_paths']\n",
    "    loaded_model['project name'] = saved_model['project_name']\n",
    "    loaded_model['categories data'] = saved_model['сategories_data']\n",
    "    loaded_model['model name']  = saved_model['model_name']\n",
    "    loaded_model['optim state dict'] = saved_model['optimizer_state_dict']\n",
    "    loaded_model['model state dict'] = saved_model['model_state_dict']\n",
    "    loaded_model['prev training stats'] = saved_model['model_stats']\n",
    "    loaded_model['img lists'] = saved_model['img_lists']\n",
    "    loaded_model['metadata'] = saved_model['metadata']\n",
    "    \n",
    "    mylogs.info(\"Model: \" + loaded_model['model name'])\n",
    "    img_channels = loaded_model['metadata']\n",
    "    num_classes = len(loaded_model['categories data'][0])\n",
    "    import resnet\n",
    "    if loaded_model['model name'] == 'ResNet18':\n",
    "            model = resnet.ResNet18(img_channels, num_classes)\n",
    "    elif loaded_model['model name'] == 'ResNet34':\n",
    "            model = resnet.ResNet34(img_channels, num_classes)\n",
    "    elif loaded_model['model name'] == 'ResNet50':\n",
    "            model = resnet.ResNet50(img_channels, num_classes)\n",
    "    elif loaded_model['model name'] == 'ResNet101':\n",
    "            model = resnet.ResNet101(img_channels, num_classes)\n",
    "    elif loaded_model['model name'] == 'ResNet152':\n",
    "            model = resnet.ResNet152(img_channels, num_classes)\n",
    "    else:\n",
    "            model = resnet.ResNet18()\n",
    "    model.load_state_dict(loaded_model['model state dict'])\n",
    "    loaded_model['model'] = model\n",
    "    model.train()\n",
    "\n",
    "    return  loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_mode = loaded_model[\"labeling mode\"]\n",
    "train_dir, val_dir, test_dir = loaded_model['img paths']\n",
    "project_name =  loaded_model['project name']\n",
    "categories_data = loaded_model['categories data']\n",
    "model_name = loaded_model['model name']\n",
    "optim_state_dict = loaded_model['optim state dict']\n",
    "model_state_dict = loaded_model['model state dict']\n",
    "prev_training_stats = loaded_model['prev training stats']\n",
    "train_img_list, val_img_list, test_img_list = loaded_model['img lists']\n",
    "img_channels = loaded_model['metadata']\n",
    "metadata = loaded_model['metadata']\n",
    "categories = categories_data[0]\n",
    "model = loaded_model['model']\n",
    "categories_weights = categories_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if categories_data[2]:\n",
    "    labels_file, columns_to_select = csv_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylogs.info(\"Binary Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dict(img_dir, columns_to_select, categories, labels_file):\n",
    "    labels_file = pd.read_csv(labels_file)\n",
    "    labels_file = labels_file.iloc[:,columns_to_select]\n",
    "    return(dict(zip(labels_file.iloc[:,0], labels_file.iloc[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgLabeler:\n",
    "    def __init__(self, label_mode, img_dir, img_list, categories, labels_file=None, columns_to_select = None):\n",
    "        self.label_mode= label_mode\n",
    "        self.img_dir = img_dir\n",
    "        self.categories = categories \n",
    "        self.img_list = img_list\n",
    "        self.labels_file = labels_file\n",
    "        \n",
    "        #### if the labels are in the csv file    \n",
    "        if self.labels_file and columns_to_select:\n",
    "            self.labels_file = csv_to_dict(columns_to_select, categories, self.labels_file)\n",
    "        #### if the labels are in the folder or file name   \n",
    "        \n",
    "        self.name_label_dic = dict.fromkeys(self.img_list, None)\n",
    "        #For each file_path its category (filename, dir or from csv) is transformed to int\n",
    "        for file_path in self.name_label_dic:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_dir = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "            #Checking if in from csv labeling\n",
    "            if self.labels_file and columns_to_select:\n",
    "                file_category_from_csv = self.labels_file[file_name]\n",
    "            else:\n",
    "                file_category_from_csv = ['No prior label']\n",
    "            for category in categories:\n",
    "                self.category_is_present = 0\n",
    "                if category == file_dir and label_mode == 'folder':\n",
    "                    self.name_label_dic[file_path] = (category, categories.index(category))\n",
    "                elif category == file_name and label_mode == 'file name':\n",
    "                    self.name_label_dic[file_path] = (category, categories.index(category))\n",
    "                elif category == file_category_from_csv and label_mode == 'csv file':\n",
    "                    self.name_label_dic[file_path] = (category, categories.index(category))\n",
    "                # if no category is found\n",
    "            if self.name_label_dic[file_path] == None:\n",
    "                self.name_label_dic[file_path] = (categories[-1], len(categories)-1)\n",
    "    def return_categories_stat(self):\n",
    "        return (self.img_dir, len(self.name_label_dic), len(self.categories) - 1)\n",
    "    def return_row(self, file_path):\n",
    "        return self.name_label_dic[file_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if categories_data[2]:\n",
    "    img_labeler_train = ImgLabeler(labeling_mode, train_dir,train_img_list, categories,  labels_file, columns_to_select)\n",
    "    img_labeler_val = ImgLabeler(labeling_mode, val_dir, val_img_list, categories,  labels_file, columns_to_select)\n",
    "else:\n",
    "    img_labeler_train = ImgLabeler(labeling_mode, train_dir,train_img_list, categories)\n",
    "    img_labeler_val = ImgLabeler(labeling_mode, val_dir,val_img_list,  categories)\n",
    "img_labeler_test = ImgLabeler(labeling_mode, test_dir,test_img_list, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylogs.info(f\"Training set{img_labeler_train.return_categories_stat()}\")\n",
    "mylogs.info(f\"Validation set{img_labeler_val.return_categories_stat()}\")\n",
    "mylogs.info(f\"Testing set{img_labeler_test.return_categories_stat()}\")\n",
    "# Refactor!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_channels == 3:\n",
    "    dimensions = ((0,0,0), (1,1,1))\n",
    "elif img_channels == 1:\n",
    "    dimensions = ((0), (1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform(dimensions):\n",
    "    return tv.Compose([\n",
    "        tv.RandomHorizontalFlip(p=0.5),\n",
    "        tv.RandomRotation(15),\n",
    "        tv.RandomCrop(204),\n",
    "        tv.RandomVerticalFlip(),\n",
    "        tv.ToTensor(),\n",
    "        tv.Normalize(dimensions[0],dimensions[1])\n",
    "    ])\n",
    "    \n",
    "    \n",
    "def get_val_transform(dimensions):\n",
    "    return tv.Compose([\n",
    "        tv.ToTensor(),\n",
    "            tv.Normalize(dimensions[0],dimensions[1])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loadDataset(Dataset):\n",
    "    def __init__(self, img_labeler, mode, transforms):\n",
    "        super().__init__()\n",
    "        self.img_labeler = img_labeler\n",
    "        self.categories = self.img_labeler.categories\n",
    "        self.imgs = self.img_labeler.img_list\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.imgs[idx]\n",
    "      ### Reading, converting and normalizing image\n",
    "        if self.mode == \"train\" or self.mode == \"val\":\n",
    "            ### Reading, converting and normalizing image\n",
    "            img = Image.open(image_path)\n",
    "            img = img.resize((224, 224))\n",
    "            \n",
    "            # obtaining label for image\n",
    "            label  = self.img_labeler.return_row(image_path)[1]\n",
    "            label = torch.tensor(label, dtype = torch.float32)\n",
    "\n",
    "            ### Apply Transforms on image\n",
    "            img = self.transforms(img)\n",
    "            return img, label\n",
    "        \n",
    "        elif self.mode == \"test\":\n",
    "            ## Reading, converting and normalizing image\n",
    "            img = Image.open(image_path)\n",
    "            img = img.resize((224, 224))\n",
    "            # if there is no prior categories\n",
    "            if self.img_labeler.return_row(image_path) == None:\n",
    "                label  = len(categories) - 1\n",
    "            else: \n",
    "                label  = self.img_labeler.return_row(image_path)[1]\n",
    "            label = torch.tensor(label, dtype = torch.float32)\n",
    "            ### Apply Transforms on image\n",
    "            img = self.transforms(img)\n",
    "            return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = loadDataset(img_labeler_train, mode = \"train\", \n",
    "                            transforms = get_train_transform(dimensions))\n",
    "val_dataset = loadDataset(img_labeler_val, mode = \"val\", \n",
    "                          transforms = get_val_transform(dimensions))\n",
    "test_dataset = loadDataset(img_labeler_test, mode = \"test\", \n",
    "                         transforms = get_val_transform(dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    num_workers = 0,\n",
    "    batch_size = 16,\n",
    "    pin_memory = True,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    num_workers = 0,\n",
    "    batch_size = 16,\n",
    "    pin_memory = True,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    num_workers = 0,\n",
    "    batch_size = 1,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preview(train_data_loader):\n",
    "    for images, labels in train_data_loader:\n",
    "        fig, ax = plt.subplots(figsize = (10, 10))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images, 4).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "dataset_preview(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preview(test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, categories_weights):\n",
    "        self.categories_weight = categories_weights\n",
    "        self.optimizer_dict = {}\n",
    "        self.optimizer_dict['optimizer'] = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "        self.optimizer_dict['lr_scheduler'] = torch.optim.lr_scheduler.StepLR(self.optimizer_dict['optimizer'], step_size = 5, gamma = 0.5)\n",
    "        self.optimizer_dict['criterion'] = nn.CrossEntropyLoss(weight=self.categories_weight)\n",
    "        \n",
    "    def return_state_dict(self):\n",
    "        return self.optimizer_dict['optimizer'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylogs.info(\"Model: \" + model_name )\n",
    "# Loading model to device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "mylogs.info(f\"Device is: {str(device)}\")\n",
    "model.to(device)\n",
    "optimizerA = Optimizer(model,categories_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, trues):\n",
    "    acc = [1 if torch.argmax(preds[i]) == trues[i] else 0 for i in range(len(preds))]\n",
    "    acc = (np.sum(acc) / len(preds))*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(data_loader, optimizer_obj, phase):\n",
    "    ## global to local\n",
    "\n",
    "    optimizer = optimizer_obj.optimizer_dict['optimizer']\n",
    "    criterion = optimizer_obj.optimizer_dict['criterion']\n",
    "    criterion = criterion.to(device)\n",
    "    lr_scheduler = optimizer_obj.optimizer_dict['criterion'] \n",
    "    ### Local Parameters\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ###Iterating over data loader\n",
    "    for images, labels in data_loader:\n",
    "        #Loading images and labels to device\n",
    "        labels = labels.to(dtype=int)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #Reseting Gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Forward\n",
    "        preds = model(images)\n",
    "        #Calculating Loss\n",
    "        _loss = criterion(preds, labels)\n",
    "        loss = _loss.item()\n",
    "        epoch_loss.append(loss)\n",
    "        #Calculating Accuracy\n",
    "        acc = accuracy(preds, labels),\n",
    "        epoch_acc.append(acc)\n",
    "        #Backward if train phase\n",
    "        if phase == \"train\":\n",
    "            _loss.backward()\n",
    "            optimizer.step()\n",
    "    ###Overall Epoch Results\n",
    "    end_time = time.time()\n",
    "    total_time = int(end_time - start_time)/60\n",
    "    ###Acc and Loss\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    epoch_acc = np.mean(epoch_acc)\n",
    "    ###Storing results to logs\n",
    "    return epoch_loss, epoch_acc, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_in_progress(phase, epoch, loss, acc, _time):\n",
    "       #Print Epoch Details\n",
    "        mylogs.info(f'{\"Training\" if phase == False else \"Validation\"} Epoch {epoch}: | Loss: {loss:.5f} | Acc: {acc:.3f} | Time: {(round(_time, 4))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer_obj, prev_train_stats, max_n_epochs,early_stop_epoch_thresh = 10):\n",
    "    start_time = time.time()\n",
    "    # initializing  training stats\n",
    "    train_logs =  prev_train_stats['best train logs']\n",
    "    val_logs = prev_train_stats['best validation logs'] \n",
    "    mylogs.info(\"Training continued\")\n",
    "    best_model = model\n",
    "    best_epoch = prev_train_stats['best epoch']\n",
    "    best_val_acc = prev_train_stats['best validation acc']\n",
    "    bad_acc_epoch = 0\n",
    "    current_epoch = prev_train_stats['current epoch'] + 1\n",
    "    best_train_logs = {}\n",
    "    best_val_logs = {}\n",
    "\n",
    "    for epoch in range(current_epoch, max_n_epochs + 1):\n",
    "        if bad_acc_epoch < early_stop_epoch_thresh:\n",
    "            current_epoch = epoch \n",
    "            ###Training\n",
    "            train_loss, train_acc, train_time = train_one_epoch(train_data_loader, optimizer_obj, \"train\")\n",
    "            #Print train Epoch Details      \n",
    "            report_in_progress(0, epoch, train_loss, train_acc, train_time)\n",
    "            #Adding stats for the epoch\n",
    "            train_logs[\"loss\"].append(train_loss)\n",
    "            train_logs[\"accuracy\"].append(train_acc)\n",
    "            train_logs[\"time\"].append(train_time)\n",
    "            ###Validation\n",
    "            val_loss, val_acc, val_time = train_one_epoch(val_data_loader, optimizer_obj, \"validation\")\n",
    "            #Print Val Epoch  Details\n",
    "            report_in_progress(1, epoch, val_loss, val_acc, val_time)\n",
    "            #adding stats on the epoch\n",
    "            val_logs[\"loss\"].append(val_loss)\n",
    "            val_logs[\"accuracy\"].append(val_acc)\n",
    "            val_logs[\"time\"].append(val_time)\n",
    "            # Early stop processing\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "                best_model_optimizer_state_dict = optimizer_obj.return_state_dict()\n",
    "                bad_acc_epoch = 0\n",
    "                best_train_logs = train_logs\n",
    "                best_val_logs = val_logs\n",
    "                best_epoch = epoch \n",
    "            else:\n",
    "                bad_acc_epoch +=1\n",
    "        else:\n",
    "            mylogs.info(f\"Early stop, best accuracy: {best_val_acc}%\")\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    mylogs.info(f\"Total time : {int(total_time/60)}\")\n",
    "    # packing training data into a nice dictionary\n",
    "    training_stats = {}\n",
    "    training_stats['train logs'] = train_logs\n",
    "    training_stats['validation logs'] = val_logs\n",
    "    training_stats['best train logs'] = best_train_logs\n",
    "    training_stats['best validation logs'] = best_val_logs\n",
    "    training_stats['best validation acc'] = best_val_acc\n",
    "    training_stats['best epoch'] = best_epoch\n",
    "    training_stats['current epoch'] = current_epoch \n",
    "# Add if not best acc during the cycle return loaded best_model_optimizer_state_dict    \n",
    "    return best_model, best_model_optimizer_state_dict, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continued training \n",
    "model, model_optimizer_state_dict, training_stats = train_model(model, optimizerA, prev_training_stats, n_of_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vis(training_stats, prev_training_stats ):\n",
    "    ### Plotting Results\n",
    "    train_logs = training_stats['train logs']\n",
    "    val_logs = training_stats['validation logs']\n",
    "    epochs = training_stats['current epoch'] + 1\n",
    "    best_epoch = training_stats['best epoch']\n",
    "\n",
    "#Loss\n",
    "    plt.title(\"Loss\", color='black')\n",
    "    plt.plot(np.arange(1, epochs), train_logs[\"loss\"], color = 'blue', label='Training')\n",
    "    plt.plot(np.arange(1, epochs), val_logs[\"loss\"], color = 'orange', label='Validation')\n",
    "    plt.axvline(x=prev_training_stats['best epoch'], color='red')\n",
    "    plt.axvline(x=best_epoch, color='green')\n",
    "    plt.xticks(color='black')\n",
    "    plt.yticks(color='black')\n",
    "    plt.xlabel(\"Epochs\", color = 'black')\n",
    "    plt.ylabel(\"Loss\", color = 'black')\n",
    "    plt.show()\n",
    "\n",
    "#Accuracy\n",
    "    plt.title(\"Accuracy\", color='black')\n",
    "    plt.plot(np.arange(1, epochs), train_logs[\"accuracy\"], color = 'blue', label='Training')\n",
    "    plt.plot(np.arange(1, epochs), val_logs[\"accuracy\"], color = 'orange', label='Validation')\n",
    "    plt.axvline(x=prev_training_stats['best epoch'], color='red')\n",
    "    plt.axvline(x=best_epoch, color='green')\n",
    "    plt.xticks(color='black')\n",
    "    plt.yticks(color='black')\n",
    "    plt.xlabel(\"Epochs\", color = 'black')\n",
    "    plt.ylabel(\"Accuracy\", color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vis(training_stats, prev_training_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best model\n",
    "def save_model(model, model_stats, model_name, best_model_optimizer_state_dict, img_channels,  categories_data):\n",
    "    ''' Save model either for inference or for continuing training '''\n",
    "    torch.save({'model_name': model_name,\n",
    "                'сategories_data':  categories_data,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'metadata': (img_channels)\n",
    "                }, f'{project_name}_{model_name}_inference.pt')\n",
    "    torch.save({'model_name': model_name,\n",
    "                'img_paths': (train_dir,val_dir, test_dir),\n",
    "                'optimizer_state_dict': model_optimizer_state_dict,\n",
    "                'labeling_mode': labeling_mode,\n",
    "                'project_name': project_name,\n",
    "                'сategories_data': categories_data,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_stats': training_stats,\n",
    "                'img_lists':(train_img_list, val_img_list, test_img_list),\n",
    "                'metadata': (img_channels)\n",
    "                }, f'{project_name}_{model_name}.pt')\n",
    "    mylogs.info(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, training_stats, model_name, model_optimizer_state_dict, img_channels, categories_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on a random test image \n",
    "for image, actual_label in test_data_loader:\n",
    "    figure, ax = plt.subplots(figsize=(6,6))\n",
    "    with torch.no_grad():\n",
    "        # Generate prediction\n",
    "        model.eval()\n",
    "        image.to('cpu')\n",
    "        model.to('cpu')\n",
    "        prediction = model(image)\n",
    "        actual_label = categories[int(actual_label)]\n",
    "        \n",
    "        top_3_probs, top_3_labels = torch.topk(prediction, 3)\n",
    "        \n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        top_3_prob_list = softmax(top_3_probs)[0].tolist()\n",
    "        top_3_labels_list = top_3_labels[0].tolist()\n",
    "        predicted_labels = (f\"{categories[top_3_labels_list[0]]} : {round(top_3_prob_list[0],3)}\\n\"\n",
    "                             f\"{categories[top_3_labels_list[1]]}:{round(top_3_prob_list[1],3)}\\n\" \n",
    "                             f\" {categories[top_3_labels_list[2]]}:{round(top_3_prob_list[2],3)}\")\n",
    "        \n",
    "        plt.title(f'Predicated label:\\n {predicted_labels} \\n\\n Actual label: {actual_label}',\n",
    "                  color= 'black')\n",
    "        plt.imshow(make_grid(image,1).permute(1,2,0))\n",
    "        break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, f1_score\n",
    "f1score_data_loader = DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    num_workers = 0,\n",
    "    batch_size = 1,\n",
    "    pin_memory = True,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ytrue_ypred_gen(f1score_data_loader, n=3000):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for image, actual_label in f1score_data_loader:\n",
    "        with torch.no_grad():\n",
    "            # Generate prediction\n",
    "            y_true.append(int(actual_label))\n",
    "            img = image.to(device)\n",
    "            model.to(device)\n",
    "            actual_label = actual_label.to(device)\n",
    "            model.eval()\n",
    "            prediction = torch.argmax(model(img))\n",
    "            y_pred.append(prediction.tolist())\n",
    "            if len(y_true) > n:\n",
    "                break\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = ytrue_ypred_gen(f1score_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
